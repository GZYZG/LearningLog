% 用于记录一些常见的机器学习，深度学习的概念

\subsection{L1/2 regularization}


\subsection{预训练，使用自编码器进行预训练}

\subsection{机器学习中常见的数据分布}

\subsection{early stopping strategy}

\subsection{Jacobean Hessian}


\subsection{不同的优化器原理}
可参考：\href{https://mp.weixin.qq.com/s/L9jCK5rtyq3fJZEBpLvagg}{从SGD到NadaMax，十种优化算法原理及实现}


\subsection{熵、相对熵、交叉熵、互信息} 
\textbf{\checkmark 2020-10-08}\\
这些概念来自信息论\cite{6773024}。简单来说，熵指的是不确定性或者信息量，熵越大不确定越大。相对熵也叫KL(Kullback-Leibler divergence)散度，用来比较两个概率分布之间的差异。不论是熵、相对熵、交叉熵，都可以看作针对某个（或多个）随机变量，对该随机变量的概率分布的一个某种熵（熵、相对熵、交叉熵）的计算。接下来就从数学上来对其进行描述。

\textbf{熵}：先介绍自信息的概念。对于某个随机变量$X$，当$X$取值为$x_0$时的自信息为$I(x_0) = -log\ p(x_0)$，即事件$x_0$发生时所带来的信息量，如果一个事件发生的概率越大，则其带来的信息量越小。熵是自信息的均值。即$X$取任一值时所能带来的期望信息量，故$X$的信息熵$H(X) = -E_{x\sim p}I(x)$（其中p是$X$所服从的分布），即$H(X) = -\sum_{x \in X}log\ p(x)$。\textbf{因为随机变量$X$会服从某个分布，假设是$p$，则$H(X)$也可以看作是概率分布$p$的信息量的期望。}

\textbf{相对熵}：也称作KL散度。KL是在信息熵的基础上定义的，用来衡量两个分布的差异，其实由上述熵的含义可知，其实KL也可以看作是随机变量$X$，其可能服从的两个分布之间的差异。假设可能服从的两个分布分别是$p, q$，则以$q$去接近$p$时的KL散度表示为：$D_{KL}(p||q) = \sum_{x \in X} p(x) log\ \frac{p(x)}{q(x)} = E_{x\sim p} log\ \frac{p(x)}{q(x)}$。经过化简可得$D_{KL} = -H(p) + \sum_{x \in X}p(x)log\ q(x)$。

\textbf{交叉熵}：\label{ce}cross-entropy，与KL散度相似，也是用来衡量随机变量$X$可能服从的两个分布$p, q$之间的差异的。其数学上的定义为：$Cross-Entropy(p, q) = -E_{x\sim p} log\ q(x) = - \sum_{x \in X}p(x)log\ q(x)$。很显然，交叉熵比KL散度多了一个$H(p)$，即$Cross-Entropy(p, q) = D_{kL}(p || q) + H(p)$。

机器学习中常使用交叉熵，既然KL散度和交叉熵都可以达到相同的目的，那为什么不使用KL散度呢？在机器学习中，上述的分布$q$常作为数据的真实分布，而$q$作为数据的预测分布，此时$H(P)$则可以视作一个常数（因为给定数据后，其真实分布是确定的），在优化模型的参数时，$H(P)$并不会对参数的优化做出贡献（不会影响优化过程），故使用交叉熵即可。





\subsection{BGD, SGD, MBGD}
神经网络的训练过程，可以看作目标函数的优化过程，在优化过程中对模型的参数进行更新，使目标函数逐步收敛到一个最优的状态。参数优化有多种方法，目前主要是基于迭代的过程，如梯度下降、牛顿法等。其中梯度下降已经霸榜多年。目前主要有以下三种梯度下降方法：
\begin{myitemize}
	\item BGD，Batch  Gradient Descent，批量梯度下降。每次计算梯度时使用全量样本。优点：
		\begin{myitemize}
			\item 所有样本都参与了梯度的计算，异常样本带来的影响更小，训练过程更稳定
			\item 收敛速度快
		\end{myitemize}
	缺点也很明显，因为使用到了所有样本，故计算耗时更长，且要将数据全部加载，对资源要求更高，且有可能收敛到局部最优。
	
	\item SGD，Stochastic Gradient Descent，随机梯度下降。每次计算梯度时只使用一个样本，通常会对全量数据进行打乱。优点：
		\begin{myitemize}
			\item 参数更新频次高，速度快。（但从另一个角度来看，单独计算每个样本的梯度或许更耗时）
			\item 可以在线优化
			\item 每次使用一个样本的随机性可能会帮助跳出局部最优
		\end{myitemize}
	缺点：每次只是用一个样本，会放大一场样本的影响，导致训练过程不稳定。
	
	\item MBGD，Mini-Batch Gradient Descent，小批量梯度下降。每次使用一部分数据计算梯度，一次通常将全量数据划分成多个batch，与SGD类似，也会对全量数据进行打乱。优点：
		\begin{myitemize}
			\item 对比BGD，速度更快，对资源要求更低；对比SGD，振荡现象没有那么明显，比SGD会更稳定
			\item 能够一定程度避免异常样本的干扰
		\end{myitemize}
	缺点：需要考虑学习率的衰减，以及选择合适的batch size，且与BGD相比存在一定程度的振荡。
\end{myitemize}
参考资料：\href{https://lumingdong.cn/summary-of-gradient-descent-algorithm.html#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%9A%84%E4%B8%89%E4%B8%AA%E5%8F%98%E7%A7%8D}{梯度下降算法的三个变种}。

\subsection{Contrastive loss, Triplet loss} 
都是一种损失函数。

Triplet loss用于训练差异较小的数据，常用于人脸识别中。以这种函数为损失函数时，输入的一个样本是一个三元组(anchor, positive, negative)，anchor是随机选择的一个样本，而positive和negative分别于anchor为同类/异类数据。在学习时，Triplet loss的目的就是让anchor的表征与positive的表征尽量靠近，与negative的表征尽量疏远。那么对于一个样本来说，Triplet loss写成公式： 
$$
\mathcal{L} = max( ||f(a)-f(p)||_2^2 - ||f(a) - f(n)||_2^2 + \alpha )
$$
公式的含义也很明显，尽量使类内数据相近，类间数据相离。

Contrastive loss是对比损失，也叫zero-one损失，主要用来处理孪生网络中的paired data的关系。通常Contrastive loss的输入是两个样本，且各自都有一个标签，Contrastive loss的目标就是：如果两个样本同类，则loss更小，否则loss更大。写成公式：
$$
\mathcal{L} = d_{ij}^2 \cdot Y_{ij} + (1 - Y_{ij} )max(margin - d_{ij}, 0)^2
$$
其中，$d_{ij}$就表示样本i, j之间的距离，这个可以有多种形式的定义，$Y_{ij}$表示样本i, j的标签是否相同，margin是一个阈值，如果$d_{ij}$超过阈值则应该尽量把它们划分开。

\subsection{Batch Normalization}
在神经网络中，前一层的输出会成为后一层的输入。当前面层的参数更新后，其输出的分布也会随之改变，经过层层的叠加，越往后的变化越大。为了拟合这些数据，深层的参数需要不断适应变化的数据，导致模型的收敛速度很慢。这里的分布指的是一层里每个神经元的分布。这种分布不一致做 \textbf{Internal Convariate Shift}（内部协变量偏移）。对于一个神经元，其取值分布会逐渐偏移，例如偏移至激活函数的饱和区，即梯度很小。

一个比较直观的方式就是对每一层的输出进行标准化。Batch Normalization 在 mini-batch 的基础上对每个神经元进行标准化，即对每个特征进行标准化。具体操作为：每一层的输入维一个 mini-batch，通过这个批次来估算每一维的均值和方差，然后对每一维进行标准化，除此之外，BN 中还对标准化后的特征进行了线性变化。
$$
\begin{array}{rlr|}
	\hline \text { Input: } & \text { Values of } x \text { over a mini-batch: } \mathcal{B}=\left\{x_{1 \ldots m}\right\} \\
	& \text { Parameters to be learned: } \gamma, \beta & \\
	\text { Output: } & \left\{y_{i}=\operatorname{BN}_{\gamma, \beta}\left(x_{i}\right)\right\} & \\
	\mu_{\mathcal{B}} & \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i} & \multicolumn{1}{l|}{\text { // mini-batch mean }} \\
	\sigma_{\mathcal{B}}^{2} & \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{\mathcal{B}}\right)^{2} & \text { // mini-batch variance } \\
	\widehat{x}_{i} & \leftarrow \frac{x_{i}-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}} & \text { // normalize } \\
	y_{i} & \leftarrow \gamma \widehat{x}_{i}+\beta \equiv \mathrm{BN}_{\gamma, \beta}\left(x_{i}\right) & \text { // scale and shift }
\end{array}
$$
注意上面公式中是以一个特征维度为例子，对于其他维也是一样的，其中的 $\gamma, \beta$ 都是需要学习的参数，即每个神经元会多出来两个参数来学习。上述过程是在针对训练而言的，但是在推断时该怎么办呢？对于测试数据，依然会进行标准化（$\gamma, \beta$ 是在训练时学习好的，所以推断时是会被固定的），但是其均值和标准差就不是通过批次数据来计算的，而是这样：
$$
\begin{aligned}
	\mathrm{E}[x] & \leftarrow \mathrm{E}_{\mathcal{B}}\left[\mu_{\mathcal{B}}\right] \\
	\operatorname{Var}[x] & \leftarrow \frac{m}{m-1} \mathrm{E}_{\mathcal{B}}\left[\sigma_{\mathcal{B}}^{2}\right]
\end{aligned}
$$
那么测试数据的标准化就是这样的：
$$
\widehat{x}=\frac{x-\mathrm{E}[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}
$$
\textbf{为什么进行了标准化还要进行线性变换呢？}如果只是进行标准化后，其很有可能落在激活函数的线性区域，例如 sigmoid 激活函数，经过标准化后基本会落在 0 左右，而这一块区域基本是线性的，而达不到激活函数的非线性的功能，因此对齐进行了缩放和偏置，使其能够落在激活函数的非线性区域，由于 $\gamma, \beta$ 是学习得到的，因此也能满足其落在线性区的需求。这样做的一个目的就是：保障每一层的表征能力。


实际情况在进行BN时，可能是在通过激活函数之前进行 BN 或者在通过激活函数后再进行BN。\textbf{通常是在激活函数之前进行 BN}，因为当输入较大时，通常激活函数的变化都较小，梯度变化不明显，故在激活函数之间就对数据进行BN，使其分布尽量稳定。

BN 的一些缺点：
\begin{itemize}
	\item 需要较大的batch以体现整体数据分布，要求 bath 的分布尽量与总体分布相近；
	\item 训练阶段需要保存每个batch的均值和方差，以求出整体均值和方差在infrence阶段使用；
	\item 不适用于可变长序列的训练，如RNN。
\end{itemize}

\subsection{Layer Normalization}
既然已经有了 BN，怎么还来了 LN 呢？参看 BN 的缺点，其中很重要的一点就是不适用于处理序列数据的网络，序列数据的格式通常为 $(batch\_size, seq\_len, emb\_dim)$，但是$seq\_len$ 并不都是相同的，。与 BN 类似，LN 也是进行标准化后再


\subsection{判别模型、生成模型}
\begin{itemize}
	\item 判别模型：直接对判别函数或者条件概率分布函数进行建模，不考虑样本的产生模型，直接研究预测模型
	\item 生成模型：学习联合概率密度$P(Y, X)$，然后求出条件概率分布$P(Y|X) = \frac{P(X, Y)}{P(X)}$，不仅要求出联合分布，还要求出训练数据的分布$P(X)$（{\color{red}{不一定要计算$p(X)$，因为对于同一个样本，计算它属于不同分类时，其$p(X)$是一样的，对判别没有帮助}}）。生成模型表示了输入$X$产生输出$Y$的生成关系
\end{itemize}

对于生成式模型，可以这样理解：\\
$p(x | y) = p(y)p(x | y)$表示的是，从$p(y)$中采样一个$y$，然后根据$p(x|y)$采样一个$x$。生成式模型希望找到那个能够使$p(x, y)$最大的$y$。1


生成模型从统计的角度表示数据的分布情况。判别模型不能反映训练数据本身的特性，但它不断寻找不同类别之间的最优分类面。

也可以从 \textbf{决策函数$Y=f(X)$或条件概率分布$P(Y|X)$} 的角度来看待判别模型和生成模型：
从数据中学习一个分类器时，希望通过给定的输入$X$输出相应的$Y$。这个模型的一般形式为：
\begin{itemize}
	\item 决策函数$Y=f(X)$：输入一个$X$就输出一个$Y$，可以将$Y$于阈值比较得到$X$的类别
	\item 条件概率分布$P(Y|X)$：输入一个$X$，输出$X$属于各个类的概率，如$P(c_1 | X), P(c_2 | X)$。取其中最大的作为$X$的类别
\end{itemize}
实际上$P(Y|X)$是隐含了或者说可以转化为决策函数形式$Y=f(X)$的。例如，将条件概率分布改写为$Y = \frac{P(c_1 | X)}{P(c_2 | X) }$。

参考资料：\href{https://blog.csdn.net/fishmemory/article/details/51711114}{判别模型(Discriminative model)和生成模型(Generative model)}、\href{https://developers.google.cn/machine-learning/gan/generative?hl=zh-cn}{Background: What is a Generative Model?}。


\subsection{Model Collapse}
模型坍塌，很形象啊，就是模型出现了一个漏洞，不管你输入什么东西都会漏进这个洞里。

这个主要出现在GAN的模型中。GAN通过生成器G和判别器D来使G捕捉到真实数据的分布。在训练GAN模型时会出现model collapse的现象，即G只捕捉到了真实数据的部分分布。为什么会这样呢？简单的介绍一下：当G捕捉到了真实数据的部分分布后，被D识破了，于是G就改变，从某个分布跳到了另外的分布，并且抛弃了原来的分布，于是就成了“猫鼠游戏”，D一直追着G跑，G最终并没有完全捕捉到真实数据的分布。参考：\href{https://blog.csdn.net/SPARKKKK/article/details/72598041}{GAN——ModeCollapse}。

\subsection{Inductive Bias}
归纳偏置，使用某个算法解决问题时所基于的假设，类似于贝叶斯中的先验（prior），与先验不同，归纳偏置在学习过程中不会被更新，而先验会不断被更新。机器学习中常见的归纳偏置：奥卡姆剃刀、CNN中的局部性、KNN中假设相似样本在特征空间中也是相邻的、SVM假设好的分类器应该是类别边界距离最大的等。

\subsection{Covariate Shift}
协变量偏移，指机器学习中训练集和测试集样本分布不一致的现象。机器学习中通常假设训练和测试数据的分布是一致的，在训练集学习的参数能否在测试数据上也有很好的表现呢？当训练集和测试集的分布不是那么相似时，covariate shift就出现了。这里的数据分布不一致举个例子：训练一个健康预测器，训练集大多是 60 岁以下的，但是测试集却大部分来自老年人。

\subsubsection{怎么发现 Covariate Shift？}
做机器学习任务，检验训练集和测试集的分布是很重要的，其实一直不太了解如何验证两个数据集的分布是否一致。验证是否发生了 Covariate Shift 其实也可以看作验证训练集和测试集的分布是否一致。既然时验证分布是否一致，那么就可以看成是一个分类任务，训练一个分类器来判断一个样本是来自训练集和测试集。具体做法：从训练集和测试集中随机挑选等量的样本，生成一个新的数据集，给这个数据集中的每个样本增加一标签，标识其来自训练集还是测试集，之后就用这个新数据集进行训练，训练完后计算模型的性能，如果性能不错，则说明出现了偏移。

\subsubsection{怎么解决？}
怎么解决呢？
\begin{itemize}
	\item 训练集和测试集的分布不一致导致模型的参数泛化性能不佳，可能是测试集中地某些样本在训练集中被“轻视”或“过度重视”了，因此可以通过附加一个权重来解决该问题。可参考：\href{https://blog.csdn.net/mao_xiao_feng/article/details/54317852}{covariate shift现象的解释}；
	\item 丢弃那些导致偏移的且不重要的特征，参考：\href{https://zhuanlan.zhihu.com/p/205183444}{Covariate Shift}。
\end{itemize}


\subsection{关于交叉熵损失函数的一点理解}
交叉熵损失函数可以从多个角度进行理解。从概率论的角度，在极大似然概率估计中，希望参数能够使得已有样本出现的概率最大。

\paragraph{二分类}
在分类任务中，由于不同样本是有标签的，样本出现的概率应该与标签对应，例如p表示1样本出现的概率，则1-p表示0样本出现的概率，那么对于1样本极大化的应该是p，对于0样本极大化的应该是1-p。

样本的极大化后的概率取对数后为相加的形式。则对于一个样本，其极大化的表示可以写成$log p$ （1样本），或者$log(1-p)$（ 0样本）。如果用一个统一的式子表示的话，可以写成$y log p + (1-y) log(1-p)$，其中y取0或1。

通常是对损失函数最小化，所以可以在极大化的表示前加个符号就变成了交叉熵损失函数：$- y log p - (1-y) log(1-p)$。将一个batch里的样本的损失累加起来就是常见的形式了。并且，在用模型计算p时，p通常通过一个函数来表示$f(x)$，x即为输入的样本，$f(x)$可以是各种机器学习模型，如逻辑回归、神经网络等。


\paragraph{多分类}
对于一个$C$个类别的多分类任务，用$p_i$表示样本$x$被预测为第$i$类的概率，$x$的真实类被为$c$，也可以用一个one-hot向量$y = [y_1, y_2, ..., y_C],\ y_i = 1\ if\ i = c\ else\ y = 0$表示$x$的真实类别。那么$x$的损失即可表示为$loss_x = \sum_{i=1}^C y_i\ \log p_i$。显然，$loss_x = \log p_c$。通常，如果$\{p_i\}_{i=1}^C$是未归一化的，那么实践中，$loss_x = \log \frac{e^{p_c}}{\sum_{i=1}^C e^{p_i}}$

除了从极大似然的角度来理解交叉熵损失函数，当然也可以直接从交叉熵\ref{ce}的角度来理解。

\subsection{关于CNN的一点理解}
于DNN相比，CNN有两个特点：1）局部感知；2）权值共享。

局部感知。在CNN中每个Feature map中的一个值只于上一层的Feature map的一小部分有关。如果将CNN展开成DNN的形式，则一层中的一个神经元的输入只是上一层的某几个神经元。与全连接不同，全连接将上一层的所有信息作为输入，捕捉整体的信息。但局部感知以某个区域内的信息作为输入，捕获上一层数据中存在的局部信息 --- 特定的模式。

权值共享。CNN中的每一个filter都可以得到一个Feature map，同一个Feature map中的元素之间通过同一个filter卷积得到。或者说，同一层的神经元由一个filter产生，共享一套参数。
见Fig.\ref{fig:share_weight}。图来源于李宏毅老师的ppt。有一点需要注意，CNN模型的输入图像可能是单通道的也可能是三通道的，此时\textbf{每个filter不仅由长和宽还有深度}，filter的深度与图像的通道数是相等的，所以一般来说有多少个filter就有多少个Feature map。

\begin{figure}[h]
	\centering
	\includegraphics[width=.3\textwidth]{pics/share_weight.png}
	\caption{CNN的权值共享}
	\label{fig:share_weight}
\end{figure}

CNN的应用非常广泛，不仅是图像，在Speech、NLP等很多领域都有非常多的应用。在决定是否要使用基于CNN的模型时，需要考虑：1）数据中存在一些较小的模式，这些模式经常出现在数据中 --- 对用卷积核；2）模式与整个数据相比比较小，更大的语义信息是由很多小的模式组成的；3）进行pooling时不会破坏数据本来的含义。

在探索CNN到底学习到了什么的时候，可以在训练好模型后，通过不断优化输入的数据，来检验是什么样的输入使得基于CNN的模型中的神经元能够得到最大的激活 --- 什么样的输入使神经元最兴奋。这些主要涉及到对CNN的解释和可视化。

\subsection{DL中的不可微操作}



\subsection{Global Max Pooling}

\subsection{Attention机制与CNN}

\subsection{学习率衰减与参数正则化}

\subsection{深度学习中常见的参数初始化方法}
深度学习依靠大量的数据进行学习能够达到很好的效果，但是这其中依靠的是大量的参数，学习就是为这些参数找到合适的值。因此，如果一开始我们就能给这些参数设置一个比较好的值，那么学习也就省力了，这也就是如何初始化的问题了。

\subsubsection{从正态分布中采样}
或许直接将参数初始化为 0 也能进行学习，但是容易存在一些问题：学习过程缓慢，难以收敛，学习效果不好，为什么呢？如果参数全为 0，则计算的损失可能会很小，能够提供的梯度值也会很小或者根本就等于 0，这对于依靠梯度更新参数的方法来说是很致命的。因此，与全 0 相比，随机初始化反而是一个不错的选择，至少不会让梯度为 0。

\subsubsection{Glorot initialization}
但是呢，从正态分布中采样也不是一个很好的方法，为什么呢？将设我们从 $\mathcal{N}(\mu, \sigma^2)$ 中采样权重，参数的方差会影响隐层的线性变换后的方差，进而影响隐层输出的方差，这会直接影响梯度的计算，如以 sigmoid 为激活函数时，则可能落在平坦的区域 --- 也就是可能会出现梯度向后传着传着就没了或者就变得很大了。

因此一个好的初始化应该满足：各层的值（线性变换后的值、激活后的值）应该有相似的方差。为了满足前向传播时各隐层的输出值有相似的方差，以及在反向传播时，梯度也具有相似的方差，我们可以推导出参数应该又怎样的方差，这其中涉及到一些数学推导，暂时不展开。结论就是，为了前向的稳定，参数的方差应该是 $\frac{1}{f_{in}}$，为了反向传播的稳定，参数的方差应该为 $\frac{1}{f_{out}}$，其中 $\boldsymbol{W}^{fan_{in} \times fan_{out}}$。那这里有两个方差怎么办：取平均，即 $Var(\boldsymbol{W}) = \frac{1}{ \frac{fan_{in} + fan_{out}}{2}}  = \frac{2}{fan_{in} + fan_{out}}$。因此如果我们从正太分布中采样，则这个正太分布的方差应该等于 $\frac{2}{fan_{in} + fan_{out}}$，均值应该等于 0，为啥呢？在推导过程中利用了方差的一些性质，要求均值为 0。如果从均匀分布 $U(a, b)$ 中采样呢？同样需要满足 0 均值，即 $a + b = 0$，那等于多少呢？因为均匀分布的方差为 $\frac{(b - a)^2}{12}$，且 $a = -b$，因此可以解得 $b = \sqrt{\frac{6}{fan_{in} + fan_{out}}}$。

上述方法就是Xavier初始化方法（又称Glorot初始化）。当然，初始化方法还要考虑激活函数的影响，Glorot 主要用于输出均值为 0 的激活函数，如 tanh。

\subsubsection{He 初始化}
如果使用 ReLu 作为激活函数，上述的 Glorot 可能就不是那么好了，为什么呢？考虑 $ReLu(x) = max(0, x)$，将隐层的输出近一半置为 0，且 ReLu 的输出均值不为 0，不满足 Glorot 推导过程中的一些条件。为了满足各层的值方差就近似，He 初始化对权重的方差变为了 Glorot 中的两倍，为什么呢？因为 ReLu 将一般元素的值置零相当于减少了一般方差。He 初始化没有同时使用 $fan_{in}, fan_{out}$，而只使用了其中一个，实验表明这已经足够了。因此，使用 He 初始化时，正态分布的方差为 $\frac{2}{fan_{in}}$，均匀分布的的 b 为 $\frac{6}{fan_{in}}$。

关于推导的一些参考：\href{https://intoli.com/blog/neural-network-initialization/}{UNDERSTANDING NEURAL NETWORK WEIGHT INITIALIZATION}、\href{https://mnsgrg.com/2017/12/21/xavier-initialization/}{Xavier Initialization}（不同激活函数下的 Glorot 初始化推导）。

\subsection{机器学习中常用的算法指标及其应用场景}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\paragraph{Accuracy, Recall, Precision, F-score}

$ACC = \frac{TP + TN}{TP+FN+TN+FP}\quad R = \frac{TP}{TP+FN}\quad P = \frac{TP}{TP+FP}\quad F(\beta) = (1 + \beta^2)\frac{P \cdot R}{\beta^2 P + R}$。混淆矩阵（Confusion Matrix）见表.\ref{tab:confusion_mat}。F-score是对R，P的一个综合评价，$\beta$度量了R 相对于 P的重要性，可以理解为$\beta = \frac{importance(R) }{importance(P)}$，则表示$\beta$越大，我们越看重R。

\begin{table}[h]
	\centering
	\caption{二分类混淆矩阵}
	\label{tab:confusion_mat}
	\begin{tabular}{|c|l|l|}
		\hline
		\multicolumn{1}{|l|}{}                          & \multicolumn{2}{c|}{Actual class (observation)}                                                                                   \\ \hline
		& tp (true positive) Correct result                          & fp (false positive) Unexpected result                                \\ \cline{2-3} 
		\multirow{-2}{*}{Predicted class (expectation)} & \cellcolor[HTML]{68CBD0}fn (false negative) Missing result & \cellcolor[HTML]{68CBD0}tn (true negative) Correct absence of result \\ \hline
	\end{tabular}
\end{table}

\paragraph*{关于$Precision$，$Recall$的选择？}这几个指标在很多任务中都有应用，，但是不同的指标侧重于不同的方面，比如P、R都有不同的侧重，但看一个指标是比较片面的，并不能反映出模型真实的效果。有可能P很高，但是R很低，而在一些场景下R是很重要的。

比如在金融风控等领域，我们希望算法能够尽量识别出所有有可能有风险的用户，这时候就侧重于 Recall，即希望算法把所有的正样本（通常有危险的，需要被找出来的被标记为正样本）都筛选出来，即使将所有样本都标为正（即 Recall=1）。因为这种情况下，可能漏掉一个正样本带来的代价是极大的，通常筛选完之后还需要交给人工进行判断。类似的场景还有癌症检测，将一个没有癌症的判断为癌症没有很大关系，但是将一个有癌症的判断为没有癌症则是很严重的，会出人命的！！！

又比如在垃圾邮件分类中，我们可能更侧重于Precision。我们希望算法在识别垃圾邮件时不要把正常邮件错分了，这个时候希望Precision尽可能高，即使将所有样本标为负也没关系（TP=0时可认为Precision=1），或者说算法只把自己十分确信为垃圾邮件的标为正，尽量降低FP，即尽量不要把正常邮件视为垃圾邮件，不然错过了offer那可咋整！！！

通常，将需要识别出的类别，或者简单的说坏的一类为正样本，为什么呢？因为好的漏掉一般不会产生啥大的影响，但是坏的跑了课就不行了！当然，也需要不同场景下选择合适的指标！！！

我们是贪心的，因此就有了一些综合的指标，比如F-score。
Precision是以被分类的所有样本为分母，Recall则是以原本所有的positives元素为分母。二者之间并没有建立直接联系，如果一个分类器，Precision很高但是Recall很低，或者Recall很高但是Precision很低，这两种分类器都是不好的，都是我们不希望的。所以我们采用F1-Score来建立Precision和Recall的联系。

\textbf{在数学中，调和平均数是永远小于等于算术均值平均数的，当用于求两个数的平均数时，如果直接用算术平均作为结果，那么两数之间的差异将被大的值削平，而调和平均数则不会极大削平这种大的差异，得到的结果更倾向于小的值}。

\paragraph{Micro-F1 \& Macro-F1}基本的F1使针对二分类任务而言的，在多分类中中，Micro-F1和Macro-F1是两种求多类别F1均值的方式。
\begin{itemize}
	\item Micro-F1：分别计算每个类别的$TP, FN, FP$，再求整体的$Recall$，$Precision$，再以整体的$P, R$来求$F1$，得到Micro-F1。在计算公式中考虑到了每个类别的数量，所以适用于数据分布不平衡的情况；但同时因为考虑到数据的数量，所以在数据极度不平衡的情况下，\textbf{数量较多的类（即常见的类）会较大的影响到F1的值}
	\item Macro-F1：分别计算每个类别的F1，再求平均，得到Macro-F1。没有考虑到数据的数量，所以会平等的看待每一类（因为每一类的precision和recall都在0-1之间），\textbf{会相对受高$Precision$和高$Recall$类（即稀有的类）的影响较大}
\end{itemize}
$Micro-F1$公式如下所示：
$$
\begin{aligned}
	 Recall_{m i} &=\frac{\sum_i TP_{i}}{\sum_i TP_{i} + \sum_i FN_{i}} \\
	Precision_{m i} &=\frac{\sum_i TP_{i}}{\sum_i TP_{i} + \sum_i FP_{i}} \\
	Micro-F1 &= \frac{ Recall_{m i} \times Precision_{m i}}{Recall_{m i}+ Precision_{m i}}
\end{aligned}
$$
$Macro-F1$如下所示：
$$
Macro-F1 &=2 \frac{ \sum_i F1_i}{N}
$$


\paragraph{ROC、AUC}接收者操作特征曲线（recevier operating characteristic curve），用于反映一个而分类器的灵敏度（sensitivity）和特异度（specificity）之间的关系。
$$
\begin{aligned}
	Sensitivity &= \frac{TP}{TP + FN}\\
	Specificity &= \frac{TN}{TN + FP}
\end{aligned}
$$
其中Sensitivity也就是TPR（True Positive Rate），也就是Recall，Specificity是TNR（True Negtive Rate）。ROC的横坐标是$1 - Specificity$，即FPR（False Positive Rate，即\textbf{负样本中有多少被分成了正样本}），纵坐标是Sensitivity。横坐标表示的是负样本中被预测为正样本的比例，纵坐标表示的是正样本中被预测为正样本的比例。

ROC如Fig.\ref{fig:roc}所示。

\begin{figure}[h]
	\centering
	\includegraphics[width=.6\textwidth]{pics/roc.png}
	\caption{ROC}
	\label{fig:roc}
\end{figure}
ROC的绘制过程：对于一个二分类问题，使用一个分类器对样本集进行预测后，可以得到每个样本属于正样本的概率，此时我们还需要一个阈值来确定那些为正样本。每选取一个阈值，就可以得到一个（TPR，FPR）数值对。当阈值从1到0不断减小时，被确定为正样本的样本数不断增大，其中TP和FP都会不断增大，由于正负样本的数量是固定的（即TPR，FPR的分母是固定的），则TPR和FPR都会不断增大。那么，
\begin{itemize}
	\item 当阈值为1时，（几乎）所有样本都为负样本，则TPR约为0，既然都为负样本那么FP也为0，则FPR也为0
	\item 当阈值为0时，所有样本都为正样本，那么肯定所有的正样本都找出来了，则TPR为1，由于所有样本都预测为正样本，那么肯定所有的负样本都预测为了正样本，则FPR为1
\end{itemize}
因此，在阈值从1到0的过程中，（TPR，FPR）不断增大，从坐标（0，0）到（1，1），如Fig.\ref{fig:threshold}所示，Fig.\ref{fig:threshold}上部分为负样本为正样本的概率的分布图（即横坐标为正样本概率值，纵坐标为对应的样本数量），下部分为正样本为正样本的概率的分布图，可见，当阈值为$B$时，大部分正样本都被分为了正样本（TP），小部分负样本被分为了正样本（FP）。由Fig.\ref{fig:roc-threshold}可见，当阈值减小时，（TPR，FPR）的变化过程。

\begin{figure}[h]
	\centering
	\subfigure[阈值变化与预测正负样本的分布]{
		\includegraphics[width=7cm]{pics/threshold.png}
		\label{fig:threshold}
	}
	\quad
	\subfigure[阈值变化与ROC]{
		\includegraphics[width=7cm]{pics/roc-threshold.png}
		\label{fig:roc-threshold}
	}
	\caption{阈值变化}
	\label{fig:threshold-change}
\end{figure}
ROC是一个曲线，那怎么作为一个指标呢？ --- 取ROC与坐标轴围成的面积，即\textbf{AUC（Area Under Curve）}。由于ROC的绘制过程，我们希望当阈值为接近0时，TPR尽量高，FPR尽量低（其实不管阈值为何值，都希望有这个效果），一个好的分类器的ROC的AUC应该尽量大。

AUC的含义：随机挑选一个正样本、一个负样本，分类器分别给出一个分数，正样本的分数大于负样本的分数的概率。\tbc{red}{强烈推荐}-相关证明：\href{http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/11/20/understanding-ROC-and-AUC}{\tbc{red}{理解 ROC 和 AUC}}。

\textbf{为什么要用ROC/AUC呢？}\newline
因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡(class imbalance)现象，即负样本比正样本多很多(或者相反)，而且测试数据中的正负样本的分布也可能随着时间变化。roc曲线不变原因：TPR和FPR是实际label内部的操作，看混淆矩阵和tpr、fpr计算公式，无论实际label比例怎么变化，tpr、fpr计算公式都是在实际为p或者n的内部计算的。AUC 关注的是样本间的排序效果。

\textbf{如何使用ROC来选择模型？}\newline
当我们有多个分类器时，给定一个数据集，可以得到多条ROC曲线，那么怎么来选择模型呢？一个很直观的想法是直接比较AUC。但是在不同场景下，我们要结合更看重的指标选择模型。如Fig.\ref{fig:roc-cmp}所示，当ROC不交叉时，可以直接选择AUC高的，当ROC交叉时则需要慎重考虑了。当需要高的Sensitiviy时，选择A，需要高Specificity（即低FPR）时选择B。


\begin{figure}[h]
	\centering
	\subfigure[ROC不交叉]{
		\includegraphics[width=7cm]{pics/roc1.png}
		\label{fig:roc1}
	}
	\quad
	\subfigure[ROC交叉]{
		\includegraphics[width=7cm]{pics/roc2.png}
		\label{fig:roc2}
	}
	\caption{ROC比较}
	\label{fig:roc-cmp}
\end{figure}

参考资料：
\begin{itemize}
	\item \href{https://blog.csdn.net/pipisorry/article/details/51788927}{分类模型评估之ROC-AUC曲线和PRC曲线}
	\item \href{https://zh.wikipedia.org/zh/ROC%E6%9B%B2%E7%BA%BF}{ROC曲线}
\end{itemize}


\paragraph{mIoU}
Mean Intersection over Union(MIoU，均交并比)，为语义分割的标准度量。其计算两个集合的交并比，在\textbf{语义分割}的问题中，这两个集合为真实值（ground truth）和预测值（predicted segmentation）。令$p_{ij}$表示实际类别为$i$，预测类别为$j$的数量，则
$$
mIoU = \frac{1}{C} \sum_{i=1}^{C} \frac{p_{ii}}{ \sum_{j=1}^{C} p_{ij} + \sum_{j=1} p_{ji} - p_{ii} }
$$
如下图Fig.\ref{fig:miou}所示：
\begin{figure}[h]
	\centering
	\includegraphics[width=.6\textwidth]{pics/miou.png}
	\caption{mIoU}
	\label{fig:miou}
\end{figure}
在\textbf{语义分割}中，被分类的对象为每个像素，真实标签为该像素所属的类别，预测标签为预测的类别。计算时，可以先计算出混淆矩阵，将对角线上的元素的值之和除以混淆矩阵中所有元素的和，再除以类别数就是mIoU了。注意，在实际计算中，要注意除零的情况。

\paragraph{Dice}
有Dice系数和Dice loss之分。
Dice系数是一种集合相似度度量函数，通常用于计算两个样本的相似度，取值范围在[0,1]，计算公式如下：
$$
dice(x, y) = \frac{2|x \cap y|}{|x| + |y|}
$$
在\textbf{语义分割}中，$x, y$可以分别代表预测的分割结果、真实的分割，分别以矩阵的形式表示。那么，计算模型的分割效果可以为：
$$
dice(pred, ground) = \frac{2(pred \cdot ground).sum()}{pred.sum() + ground.sum()}
$$
其中，$\cdot$和\textit{.sum()}分别表示矩阵的逐元素乘积、逐元素求和。
Dice loss则是：$1 - dice(pred, ground)$，Dice loss 首次在VNet中提出。

在图像分割实践中，可以用Dice loss或者交叉熵损失函数作为目标函数，但是由于交叉损失函数的梯度形式更优，更倾向于选择交叉熵损失函数。
Dice Loss特点：
\begin{itemize}
	\item 训练误差曲线非常混乱，很难看出关于收敛的信息。尽管可以检查在验证集上的误差来避开此问题
	\item Dice Loss比较\textbf{适用于样本极度不均的情况}，一般的情况下，使用 Dice Loss 会对反向传播造成不利的影响，容易使训练变得不稳定
	\item Dice对mask的内部填充比较敏感
	
\end{itemize}
作为Dice loss的一个替代，可以使用cross entropy loss。原因：
\begin{center}
	使用交叉熵做损失函数时，在反向传播时，计算得到的梯度的形式是类似于（p-t）的，其中$p, t$分别是预测值和标签。而dice loss，如果将其写成$\frac{2pt}{p^2+t^2}$ 或 $\frac{2pt}{p+t}$的形式，则在反向传播时，梯度大概时这个样子的： $\frac{2t(t^2-p^2)}{(p^2+t^2)^2}$ 或 $\frac{2t^2}{(p+t)^2}$。这样的梯度有什么问题呢？当$p, t$都很小时，梯度可能会变得很大，这也是\textbf{dice loss在训练过程中不稳定的原因}了。
\end{center}



\paragraph{Rand Error}
Rand Error是以Rand Index（兰德系数）为基础的。Rand Index用于衡量两个数据簇之间的相似性。Rand Index的定义：
对数据点集进行分类时，用a表示实际为同一类，预测时也为同一类的数据点对（pair）的数量，b表示实际为不同类，预测时也为不同类的数据点pair的数量，则
%\binom{n}{k}\qquad\mathrm{C}_n^k
$$
RI(Rand Index) = \frac{a + b}{\mathrm{C}_n^k}
$$
Rand Error定义为：
$$
RE = 1 - RI
$$
RE可以用于衡量图像分割算法的效果。可以参考：\href{http://www.otlet-institute.org/wikics/Clustering_Problems.html#toc-Subsection-4.1}{Rand Index计算}。

\paragraph{Hausdorff 距离} 可以用于衡量两个点集之间的距离，定义如下，其中$\boldsymbol{X}, \boldsymbol{Y}, d(x, y)$表示两个点集和点之间的距离度量函数：
$$
d_{H}(X, Y) = \max (d_{X Y}, d_{Y x}) = \max \left\{\underset{x \in X}{\max } \min _{y \in Y}d(x, y),\quad \max _{y \in Y} \min _{x \in X} d(x, y)\right\}
$$
\textbf{Dice缺陷在于对边界的刻画不敏感}，注意力主要集中在mask的内部。而Hausdorff距离作为形状相似性的一种度量，能够为Dice做出较好的补充。
\begin{figure}[h]
	\centering
	\includegraphics[width=.5\textwidth]{pics/Hausdorff distance.png}
	\caption{Hausdorff Distance}
	\label{fig: hausdorff distance}
\end{figure}

\paragraph{MRR}
Mean Reciprocal Rank，常用来衡量搜索算法效果的指标，目前被广泛用在允许返回多个结果的问题，或者目前还比较难以解决的问题中（由于如果只返回top 1的结果，准确率或召回率会很差，所以在技术不成熟的情况下，先返回多个结果）。在这类问题中，系统会对每一个返回的结果给一个置信度（打分），然后根据置信度排序，将得分高的结果排在前面返回。核心思想很简单：返回的结果集的优劣，跟第一个正确答案的位置有关，第一个正确答案越靠前，结果越好。定义如下：
$$
MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{rank_i}
$$
其中$Q$为查询集合，$rank_i$是第$i$个查询的结果集中正确结果的排名。

\paragraph{AP}
Average Precision，平均精确率。对于二分类问题，给定样本真实标签$\{y_1, ..., y_n\}$和模型预测的正样本置信度$\{c_1, ..., c_n\}$，计算AP：
\begin{enumerate}
	\item 按照置信度从大到小对样本进行排序，令排序后的样本的置信度为$\{c_1, ..., c_n\}$
	\item 令 $i=1$，重复执行：
	\begin{enumerate}
		\item 以$c_i$为阈值，得到预测的正负样本，即前$i$行预测为正样本，之后的均为负样本
		\item 计算当前阈值下的Recall和Precision
		\item $i += 1$
		\item 当$i > n$则结束循环
	\end{enumerate}
	\item 排序后的每个样本都对应一个(Recall, Precision)对，即$\{(r_1, p_1), ..., (r_n, p_n)\}$
	\item 上一步得到的Recall列表$\{r_1, ..., r_n\}$中的元素$r_i$与$r_{i-1}$做差，设$r_0 = 0$，则可以得到$\{d_1, ..., d_n\}$，其中$d_i = r_i - r_{i-1}$
	\item 求和：$AP = \sum_{i=1}^n d_i \cdot p_i$
\end{enumerate}
如Fig.\ref{fig:ap}所示，该表是按照置信度排序后的样本，correct列表示该样本的真实标签，P、R列是按照上述方式计算出来的Recall和Precision。
\begin{figure}[h]
	\centering
	\includegraphics[width=.3\textwidth]{pics/AP.png}
	\caption{AP计算示例}
	\label{fig:ap}
\end{figure}

\paragraph{MAP}
Mean Average Precision，是AP的均值。AP通常是针对单个类别而言的，当有多个类别时，分别计算每个类别的AP，再进行算术平均。

\tbc{red}{注意：}AP和MAP常用在目标检测和信息检索领域中。
\begin{itemize}
	\item 目标检测领域中，对于一个类别，可能会检测出多个检测框，每个检测框可以根据IoU来判断检测框的真实标签是1还是0（针对一个类别的检测而言，\tbc{green}{因为目标检测中的Ground Truth也是一个边界框，所以不需要和边界框完全一致才作为正样本，只要IoU大于一定阈值即可}），每个检测框还对应一个置信度。此时即可按照上述方式计算该类别的AP
	\item 在信息检索领域，对于一个查询$q$，通常希望模型能够给出top K个结果（\tbc{red}{有序的}）。若$q$真实的有序搜索结果列表为$\{d_1, ..., d_m\}$，则可以对这K个结果的标签，由于模型的输出已经是排好序的，故可以直接计算AP。对于多个查询，则计算每个查询的AP后再取平均
\end{itemize}
总而言之，根据某种方法判定样本的真实标签（如目标检测中通过IoU判定、分类任务中给定的标签、搜索中给定的真实搜索列表等），再根据置信度进行排序（如目标检测中的置信度、分类任务中输出的分数、搜索中直接给出的排序等），计算每个位置处的$(recall, precision)$，按照上述方式即可算出AP。


\paragraph{DCG、NDCG}
Discounted Cumulative Gain，折扣累计增益。在介绍DCG之前有必要先介绍一下CG，Cumulative Gain，即累计增益。这两个指标主要用于搜索领域。对于模型返回的$p$个结果（\tbc{red}{有序的}），每个位置处的结果与查询的相关性为$rel_i$，则该结果的CG为：
$$
CG_p = \sum_{i=1}^p rel_i
$$
很明显，CG没有考虑结果的先后顺序，在搜索中，结果的顺序是至关重要的，因此产生了DCG：同一个相关度，排名越后则增益越小，即与所处排名成反比。
$$
DCG = \sum_{i=1}^p \frac{rel_i}{\log_2(i+1)}\qquad or\qquad \sum_{i+1}^p \frac{2^{rel_i} - 1}{\log_2(i+1)}
$$
通常，不同的查询对应的结果列表是不一样长的（\tbc{green}{不同长度的搜索结果对应的DCG值范围不一样，无法直接比较，例如长的结果列表DCG最大可为10，短的最大可为5，前者的DCG为4和后者的DCG为4并不代表二者的结果列表质量一样}），因此不能将DCG用于评价结果列表长度不同的查询效果，因此需要对DCG进行归一化，即NDCG（Normalized DCG，归一化折扣累计增益），
$$
NDCG = \frac{DCG}{IDCG}
$$
其中IDCG为理想情况下的折扣累计增益，表示真实的结果列表的DCG，计算方式为取真实结果列表的前$p$个结果计算DCG，即
$$
IDCG = \sum_{i=1}^p \frac{2^{rel_i} - 1}{\log_2(i+1)}
$$

\tbc{red}{注意：}MAP和NDCG都可以用于衡量搜索结果的质量，但是MAP只支持两种相关性：\{相关，不相关\}，NDCG可以支持多种相关性得分，如1-5。


\paragraph{肯德尔系数}

\paragraph{Set Based Measure}

\subsection{深度学习模型中特殊的结构}
\paragraph{Residual learning block}
跳跃连接，基本的残差块如Fig.\ref{fig:residual}所示（当然残差块不止有这一种形式，可以根据需求定义不同的残差块）。残差学习由何凯明基于以下问题提出：给定一个学习问题后，逐渐加深网络的层的时候，模型的效果应该是逐渐提升的或者不能低于原来模型的效果，但是在实验中发现通常加深后模型的效果反而变差了。按理来说就算不能提升了，额外增加的层也可以学习到一个恒等映射来保持效果不变啊，但是为什么反而下降了呢？这便是\textbf{模型退化问题}。

假设原本要学习的问题是$\mathcal{H}(x)$，之前的想法是直接学习它，在残差学习中，将它进行分解$\mathcal{H}(x) = \mathcal{F}(x) + x$，由于$x$是已知的，那么只需要学习$\mathcal{F}$就好了，$\mathcal{F}$也就是所说的残差。

\textbf{为什么这样会有效呢？}由于神经网络中通常都会使用非线性函数来拟合复杂的函数，但是对于线性关系却有点力不从心（可能这就是为什么不能学习到恒等映射的原因吧）。残差学习不仅保留了学习非线性函数的能力，也提高了线性函数的学习能力 --- $\mathcal{F}$为0即可。Resudial Learning的这种能力使得更深的网络称为可能。

\begin{figure}[h]
	\centering
	\includegraphics[width=.4\textwidth]{pics/Residual.png}
	\caption{Residual}
	\label{fig:residual}
\end{figure}

\paragraph{Dense Connection}
稠密连接，如Fig.\ref{fig:dense block}所示。每一层与之前的所有层都有连结。用$H_l$表示第$l$层，$\boldsymbol{X}_l$表示第$l$层的输出，则$\boldsymbol{X}_l = H_l(\boldsymbol{X}_0, \boldsymbol{X}_1, ..., \boldsymbol{X}_{l-1})$。

有以下优点：1）减轻了梯度消失问题；2）加强了特征的传播，能够有效的利用学习到的特征；3）能够利用多层次的特征。
\begin{figure}[h]
	\centering
	\includegraphics[width=.4\textwidth]{pics/dense block.png}
	\caption{Dense Block}
	\label{fig:dense block}
\end{figure}

\paragraph{Dilated Convolution}空洞卷积。在pooling时，可以减小feature map的尺寸，也能增大每个元素的感受野但是也损失了空间信息。在进行分割时，pooling损失的那部分信息是难以复原的。dilated卷积是一种特殊的卷积，与通常的卷积不同，dilated卷积会在\textbf{卷积核元素之间插入空格}，其实相当于一个更大的卷积核，而那些插入的卷积核的值一直为0。如Fig.\ref{fig:dilation}所示。Dilation卷积可以在不做pooling损失信息的情况下增大感受野，pooling虽然可以增大感受野但是失去了位置信息，难以从pooling后的层恢复到原来的信息，而dilation卷积不仅增大了感受野，还保留了特征图中元素的相对空间信息。
\begin{figure}[h]
	\centering
	\includegraphics[width=.8\textwidth]{pics/dilation.jpg}
	\caption{Dilation Convolution}
	\label{fig:dilation}
\end{figure}

\paragraph{1 $\times$ 1 Conv}
$1\times 1$卷积很显然，就是卷积核的长宽均为1，用$c_i$ 和$c_o$  分别表示输入和输出通道的数目，为了获得多个通道的输出，可以为每个输出通道创建一个形状为$c_i\times 1 \times 1$ 的卷积核张量，这样卷积核的形状是$c_o \times c_i \times 1 \times 1$。

由于$1\times 1$卷积的长宽为1，无法关注到周围像素的信息，只能关注到同一位置不同通道上的信息，其作用主要由以下几点：
\begin{itemize}
	\item 融合多个通道间的信息
	\item 在不改变feature map尺寸的情况下改变feature map的通道数，例如在图像分割中
	\item 降低参数量
	\item 在以每像素为基础应用时，$1\times 1$卷积相当于全连接层
\end{itemize}
更多解释可参考动手学深度学习：\href{https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/channels.html#times-1}{$1\times 1$  卷积层}

\subsection{常用数据增强手段}
增强之前，先想一想：真的需要增强数据吗（通常来说是的）？需要增加的多少数据？需要增加什么样的数据（并不是什么样的数据都可以，主要考虑应用场景中一般会出现的数据即可）？
\paragraph{仿射变换}
仿射变换（Affine Transformation）是指在二维向量空间中进行一次线性变换(乘以一个矩阵)和一次平移(加上一个向量)，变换到另一个向量空间的过程。
$$
\left[\begin{array}{l}
	u \\
	v \\
	1
\end{array}\right]=\left[\begin{array}{ccc}
	a_{1} & b_{1} & c_{1} \\
	a_{2} & b_{2} & c_{2} \\
	0 & 0 & 1
\end{array}\right]\left[\begin{array}{l}
	x \\
	y \\
	1
\end{array}\right]
$$

图解，放射变换的种类也如Fig.\ref{fig:affine}所示：
\begin{figure}[h]
	\centering
	\includegraphics[width=.8\textwidth]{pics/affine.png}
	\label{fig:affine}
	\caption{仿射变换}
\end{figure}

\paragraph{弹性形变\cite{simard2003best}}
最早是从UNet中了解到弹性形变，在细胞分割中，弹性形变发挥了重要作用；弹性形变也用在手写数字识别中。可以发现，在这两种任务中，任务所涉及的对象并不是刚体，简单的仿射变换并不能满足我们的需求。弹性形变所针对的数据特点：对象不是刚体，可能在不同的场景下会有形变。

弹性形变流程：
\begin{itemize}
	\item 对图像imageA进行仿射变换，得到imageB
	\item 对imageB图像中的每个像素点随机生成一个在x和y方向的位移，$\Delta \mathrm{x}$和$\Delta \mathrm{y}$。其位移范围在(-1, 1)之间，得到一个随机位移场(random displacement fields)
	\item 用服从高斯分布的$N(0, \delta)$对step2中生成的随机位移场进行卷积操作(和CNN中的卷积操作一样，说白了就是滤波操作)。我们知道δ越大，产生的图像越平滑。下图是论文中的不同δ值对随机位移场的影响，下图左上角为原图，右上角为$\delta$较小的情况(可以发现，位移方向非常随机)，左下角和右下角为较大的不同$\delta$值
	\item 用一个控制因子$\alpha$与随机位移场相乘，用以控制其变形强度
	\item 将随机位移场施加到原图上，具体是\textbf{怎么施加的呢}？首先，生成一个和imageB大小一样的meshgrid网格meshB，网格中的每个值就是像素的坐标，比如说meshgrid网格大小为512x512，则meshgrid中的值为(0, 0), (0, 1), ..., (511, 0), (511, 511)，然后将随机位移场和meshB网格相加，这就模拟了imageB中的每个像素点在经过随机位移场的作用后，被偏移的位置，meshB与随机位移场相加后的结果记做imageC
	\item 弹性变形最终输出的imageC中每个位置的灰度值大小，组成一副变形图像，现在imageC中每个像素点存储的是$(\mathrm{x}+\Delta \mathrm{x}, \mathrm{y}+\Delta \mathrm{y})$，如下图中的$\mathrm{A}^{\prime}$，那怎么转化成灰度值呢，依据论文，作者是根据imageB中的B位置的双线性插值灰度值作为$\mathrm{A}^{\prime}$点的像素灰度值大小（如Fig.\ref{fig:elastic-deform}所示），最终将imageC输出得到变形图像
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[width=.8\textwidth]{pics/elastic_deform.png}
	\label{fig:elastic-deform}
	\caption{弹性形变}
\end{figure}
参考：\href{https://zhuanlan.zhihu.com/p/342274228}{数据增强：弹性变形(Elastic Distortion)}。

\paragraph{增加噪声}
如椒盐噪声。

\paragraph{GAN}


\subsection{方差与偏差}
\paragraph{偏差（Bias）}通常是指：对于一种模型，它的平均（训练多次也就得到了多个模型实例）预测结果与真实值之间的差距。偏差度量的是学习到的函数与真实函数的差距的期望，即$E[\hat{f} - f]$。

\paragraph{方差（Variance）}选用一种模型（如KNN，SVM，NN等），使用不同的数据集可以得到该种模型的多个实例（即训练好参数的模型）。\textbf{这些模型}对于同一个输入给出的预测值的方差，即$E[(\hat{f} - E[\hat{f}])^2]$。方差是受所使用的训练集影响的，我们使用不同分布的数据集训练出来的模型会导致学出来的模型参数变化，这反映出来的就是针对同样的输入会产生不一样的预测值，这些预测值的方差就反映了模型的方差。

在模型很复杂的时候，在训练集上学习的模型能够准确地预测，产生较小的偏差，但是更容易产生大方差，因为模型有更多的参数，能够“死记硬背”记下输入与输出的映射关系，这个时候模型可能记住了一些没用的东西甚至是噪声，而模型是没有见过测试集的，稍微有些不同的数据点的输出可能会有较大的差别，从而产生大的偏差。 --- \textbf{过拟合}

当模型较简单的时候，可能无法充分地利用训练集，在测试集上会产生较大的偏差，但是因为模型参数少，模型关注地特征较少，光从这些特征来看的话，测试集中的数据与训练集中的数据会有更高的相似性，因此就算不同的数据输入，但是由于某些特征被忽略后数据反而是相似的，所以模型给出的输出是类似的，因而方差小。--- \textbf{欠拟合}

参考：\href{http://scott.fortmann-roe.com/docs/BiasVariance.html}{Understanding the Bias-Variance Tradeoff}，\href{http://www.r2d3.us/visual-intro-to-machine-learning-part-2/}{Model Tuning and
	the Bias-Variance Tradeoff}（可视化解释，墙裂推荐），\href{https://www.cnblogs.com/makefile/p/bias-var.html}{偏差方差分解}（分解过程的详细推导）。


\textbf{注意：}\textbf{一种模型相当于一个函数空间}，当我们选择一种模型进行训练，得到了该种模型的各个参数，就相当于得到了一个实例化的模型。模型种类相当于类，训练好的模型相当于对象。喂数据训练模型时相当于在某个特定的函数空间中找到一个合适的函数 --- 即我们要的模型。有时候训练后的模型效果不好，可能不是这种模型不行，可能是没有在这个函数空间中找到合适的函数。

\subsection{类别不平衡问题}
指的是监督学习中，不同类别的样本数目具有较大的差异（样数据分布与均匀分布差异较大）。
\paragraph{类别不均衡可能造成的问题}
\begin{itemize}
	\item 一些评价指标可能失效。例如在癌症检测中，可能$99\%$的样本都是0，只有$1\%$的样本为1，这个时候即使将所有样本预测为0也能有很好的acc，但是漏检是很严重的！类别不平衡使得一些评价指标并不能反映模型真是的能力
	\item 
\end{itemize}

\paragraph{决解办法}
\subparagraph{数据角度}
中心思想就是直接改变数据分布。
\begin{itemize}
	\item 获取更多数据，使数据分布趋向于均衡
	\item 上采样。通过一些方法，使得占少数的类别（minority类）的样本数增加，常用的方法：
	\begin{itemize}
		\item 重复采样minority类，使其样本数增加
		\item 合成的方法。根据已有数据集生成新的样本，如SMOTE方法及其变体
		\item 基于聚类。分别对major和minority类进行聚类，再通过过采样的方法使得major和majority中各个簇的数量相等，例如原本major类聚类后样本数目比为1:2:3，minority类聚类后为1:2，通过过采样的方法，先使majority与minority样本数相等，再使类内部各个簇的数目相等。这样不仅可以解决类别间的不平衡，还可以解决类内部的不平衡
	\end{itemize}
	\item 下采样。通过一些方法把占多数的类别（major类）的样本数降低。下采样的方法有很多
	\begin{itemize}
		\item 随机下采样，从major类中随机保留一部分样本
		\item 基于临近样本，来选择保留哪些major类样本
		\item 基于聚类，对major类进行聚类，使其具有N（minority类样本数）个簇，用这N个簇的中心作为major类采样后的样本
	\end{itemize}
\end{itemize}

\subparagraph{算法角度}
\begin{itemize}
	\item 选择对数据倾斜相对不敏感的算法，如树模型等
	\item 在下采样时会损失一部分信息，可以从major类中采样多个数据集来学习不同的模型，即集成学习（Ensemble集成算法）。首先从多数类中独立随机抽取出若干子集，将每个子集与少数类数据联合起来训练生成多个基分类器，再加权组成新的分类器，如加法模型、Adaboost、随机森林等
	\item 将任务转换成异常检测问题。譬如有这样一个项目，需要从高压线的航拍图片中，将松动的螺丝/零件判断为待检测站点，即负样本，其他作为正样本，这样来看，数据倾斜是非常严重的，而且在图像质量一般的情况下小物体检测的难度较大，所以不如将其转换为无监督的异常检测算法，不用过多的去考虑将数据转换为平衡问题来解决
\end{itemize}

\subparagraph{评价指标角度}
\begin{itemize}
	\item 混淆矩阵
	\item F-score
\end{itemize}

\subsection{交叉验证}
在训练模型的时候，通过会把数据集划分为训练集和测试集，测试集的作用用于学习模型参数，测试集是为了检验模型在未见过的数据上的效果。但是只将数据集划分为测试集和训练集有以下问题：
\begin{itemize}
	\item 最终模型与参数的选取将极大程度依赖于你对训练集和测试集的划分方法。不同的划分情况，学习出来的参数是不一样的；固定模型参数，模型在不同划分上的表现也是不一样的。这种情况使得我们无法准确对模型的能力进行评估，不利于我们选择最优的模型（指何种模型）以及最优的模型参数（一般是指超参数，而不是模型学习到的参数）；
	\item 该方法只用了部分数据进行模型的训练，无法充分利用已有的数据。测试的效果只是针对某个划分，并不是针对整个数据集，不够有说服力；
\end{itemize}
为了解决以上为题，即1）选择最好的模型与参数；2）充分利用数据，使模型的测试结果有说服力，就有了交叉验证（Cross Validation）。常见的交叉验证方法：
\begin{itemize}
	\item LOOCV（Leave-one-out cross-validation），即留一验证。对于有N个样本的数据集，重复N次，每次选择其中一个作为测试集，其他的作为训练集，这样就得到了N个$(train_i, test_i), i = 1, ..., N$训练集、测试集对儿。分别用这N个训练集-测试集对儿来训练模型，这样就可以学习到N个模型。每个模型都可以得到一个测试得分，进行平均后就作为这类模型在这个数据集上的测试分数
	
	\item K-fold CV，即K折交叉验证。与LOOCV类似，但是对数据集的划分不同，是把数据集划分为K份，每次取其中一份作为测试其，其余作为训练集，这样就可以得到K个测试得分，进行平均后作为最终的测试分数
\end{itemize}
\textbf{注意：给定模型种类和一组超参数，这样就确定了一个模型，但是模型的参数需要通过数据来学习（如Fig.\ref{fig:model}所示），比如线性回归中的权重，上述的N个或K个模型是针对某个模型种类和一组超参数组合而言，K或N个不同的训练集学习到的K或N个模型，但它们的超参数都是一样的}，具体过程就是：\tbc{red}{确定模型类别 ==> 确定超参数 ==> 喂数据进行学习（喂K次不同的数据） ==> 得到K个模型（但是超参数都一样） ==> 计算平均得分 ==> 得到这种模型在这种超参数下的得分}，这也就是CV可以用于选择模型（\textbf{选择模型最优的超参数}）的原因。

\begin{figure}[h]
	\centering
	\includegraphics[width=.5\textwidth]{pics/model.png}
	\label{fig:model}
	\caption{模型的超参数与学习参数}
\end{figure}

一些值得注意的问题：
\begin{itemize}
	\item LOOCV计算成本太高而且不同训练集之间的重合度太高
	\item K的选取。K太大，投入的训练集也大，得到的模型可能会有较小的偏差，但是由于训练集之间的重叠度较高，会存在较高的方差
\end{itemize}


\subsection{归一化 vs 标准化 定量的分析}
参考：\href{https://mp.weixin.qq.com/s/lO3Li7dWAvtzefVA_M0dJg}{https://mp.weixin.qq.com/s/lO3Li7dWAvtzefVA\_M0dJg}\\
\textbf{归一化：}将值变换到$[0, 1]$或者$[-1, 1]$之间，使得不同类型的特征之间有了可比性，对利用了样本之间距离的算法很重要。\textbf{标准化：}改变数据的统计特征，如将均值和方差变为0和1。在现实中，一个变量极有可能是服从正态分布的。


一些重要的结论：
\begin{itemize}
	\item 对于不同的算法，可能不同的缩放方法有不同的效果，例如Tree-based模型对特征的尺度是不敏感的，而一些依赖样本之间的距离的方法则对尺度很敏感，如KNN
	\item 归一化可能会将数据点挤压到一起
\end{itemize}

\subsection{学习率调整策略}
\paragraph{WHY?}学习率（Learning Rate, LR）是基于梯度下降的优化算法中不可或缺的一部分，目前的深度学习模型无一不使用梯度来优化模型参数，设置学习率是不可避免的。在训练过程中，学习率是否应该变化，应该如何变化对模型的收敛效果有很大的影响。看看LR对学习过程的影响：
\begin{myitemize}
	\item 学习率设置太小，需要花费过多的时间来收敛
	\item 学习率设置较大，在最小值附近震荡却无法收敛到最小值
	\item 进入局部极值点
	\item 停在鞍点处，不能够在另一维度继续下降
\end{myitemize}

\paragraph{哪些策略}总体来说，LR可以是固定的，即整个训练过程中不变；也可以是动态的，即在训练时变化，当让根据不同的变化方式动态策略又有多种。
\begin{myitemize}
	\item 固定LR，整个训练过程中LR保持不变。固定LR适用于优化目标为凸函数的情况，当为非凸时可能会收敛到局部极值点然后开始振荡。且LR的大小选择很重要，为了保证收敛一般会设置的比较小。
	\item LR衰减（LR Decay），即LR随着训练进程而减小。在训练初期大步长向目标前进，后期小碎步靠近最优点。常见的衰减策略：
	\begin{myitemize}
		\item 固定步长衰减，即每隔一定步数/epoch数，就对LR衰减一次
		\item 指数形式衰减，即以指数形式衰减，通常是$LR = LR \times \gamma^{n}$，其中$\gamma$由用户指定，$n$可以是\textit{step}或\textit{epoch}
		\item 多步长衰减，即在不同的区间使用不同的LR。通常是设置一系列的epoch锚点，达到锚点后LR就衰减一次，因此也要设置$\gamma$
	\end{myitemize}
	\item 基于Armijo准则步长搜索算法，遵循两个准则搜索步长：1）目标函数值下降幅度要大于一定阈值；2）搜索的步长不应该太小。这种步长调整策略较为耗时
	\item 循环学习率，即LR的变化呈现周期性，当然也可以每过一个周期LR就衰减一次
	\item 自适应学习率，即自定义一些规则来决定学习率的如何改变
	\item 不同的网络层使用不同的LR
\end{myitemize}
以上这些LR调整策略只是简单介绍了调整策略的思想，在实践中，深度学习框架一般会提供一些LR调整策略的接口，也可以自定义调整策略。

参考资料：\href{https://lumingdong.cn/setting-strategy-of-gradient-descent-learning-rate.html}{梯度下降学习率的设定策略}。

\subsection{Batch Size与Learning Rate}
模型训练过程中，\textbf{Batch Size}和\textbf{Learning Rate}是两个不可不调的参数。


\subsection{深度学习的梯度下降优化算法}

\subsection{数据探索}
\paragraph{What?}数据探索是机器学习任务中的一个过程，通过数据探索，我们应该能够对数据有一个深入的认识。很多人认为数据探索只是任务开始之前的一个步骤，其实数据探索应该\textbf{贯穿整个任务的生命周期}。在任务开始前，对数据有个整体的把握，知道数据的形式、数据量、数据的异常、缺失值、冗余等；在完成任务的过程中，发现当前使用的特征的特点，特征的价值、关联等；完成任务后，分析特征的重要性、误差，以及可能的优化方向。\textbf{数据探索，是我们选用何种算法的依据，以及算法模型中使用哪些特征。}

\paragraph{How?}
\subparagraph{数据初探}认识数据，掌握原始数据的基本情况，通过这一步，我们要做到：
\begin{itemize}
	\item 数据集的基本情况：数据集有多大、数据的形式、数据的类型
	\item 重复值、缺失值、异常值：去除重复值，缺失值的处理，缺失是否严重、是否有特殊含义，发现异常值并处理
	\item 特征之间的冗余性：是否有特征重复表达了
	\item 是否存在时间信息：引入时间后，处理会更复杂，通常要进行相关性、趋势、周期和异常点的分析，以及潜在的数据穿越问题
	\item 标签分布：分类问题中类别分布是否均衡、回归问题中是否有异常值、值的分布情况、\textbf{是否需要进行目标转换}
	\item 训练集与测试集的分布：训练集与测试集中标签的分布是否一致、两个数据集是否同分布
	\item 单变量/多变量分析：特征的分布、特征之间的关系、特征与目标变量的关系
\end{itemize}
了解数据长什么样后，我们才能知道\textbf{该选用哪个算法模型}、\textbf{该如何处理数据}、对数据进转换。

\subparagraph{变量分析}分析特征的特点、特征之间的关联，冗余性等。主要可以分成：
\begin{itemize}
	\item 单变量分析：按照变量的取值，可以将变量分为连续性和离散型。连续性一般为数值，离散型则比较多了，如类别型、字符串、日期等。单变量分析分析标签、特征的分布，特征与目标的相关性等。挖掘特征图用目标的关联，可以帮助去除无效的特征，\textbf{找到有效的特征组合（特征交叉）}，如强相关加弱相关、强相关加强相关等
	\item 多变量分析：分析特征变量之间的关系。帮助我们发现哪些特征是冗余的，进行特征组合，发掘更高阶的特征
\end{itemize}

\subparagraph{模型分析}任务完成或者模型训练完后，一般还需要对算法进行调整，这时候就需要通过分析正在使用的模型有哪些缺点，主要的方式有：
\begin{itemize}
	\item 学习曲线：发现是否存在过/欠拟合的问题
	\item 特征重要性分析：通常在得到模型后，我们可以获得特征的重要性，分析特征的重要性是否符合实际情况、或者发现不符合直觉的深层现象、帮助我们选择特征
	\item 误差分析：通过模型预测结果发现问题。回归中看预测结果的分布，分类中看混淆矩阵等。借此来发现对于哪些样本表现不佳，找到模型的弱点，或者\textbf{为什么在这些样本上不佳的原因，有时我们需要赋予这些 hard 样本更高的权重}
\end{itemize}

