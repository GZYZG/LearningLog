\subsubsection{马尔科夫链/隐马尔科夫链}
马尔可夫模型主要用于研究时间序列的分布的，若已有一个是时间序列：$X_0, X_1, X_2, ..., X_n$，马尔可夫模型要解决的就是这些随机变量的取值是如何随着时间而变化的、每个随机变量取值的概率 --- 这些随机变量取值的分布。随机变量的取值可以称为状态。那么问题就成了：
$$
P(X_0=s_0, X_1=s_2, ..., X_n=s_m) = ?
$$
或者说，下一个时刻的随机变量的取值问题：
$$
P(X_n | X_0=s_0, X_1=s_2, ..., X_{n-1}=s_{m-1}) = ?
$$
为了简化这个问题，有了马尔科夫假设。

（1阶）马尔可夫假设：当前状态的取值只取决于前一个时刻的状态。即：
$$
P(X_n | X_0=s_0, X_1=s_2, ..., X_{n-1}=s_{m-1}) = P(X_n | X_{n-1}=s_{m-1})
$$
满足这样性质的一系列随机变量串联在一起就是马尔科夫链了。

那隐马尔科夫链与马尔科夫链又有什么关系呢？
隐马尔科夫模型描述的是两个时序序列的联合分布：$p( \boldsymbol{X}, \boldsymbol{Y} )$，其中$\boldsymbol{X}, \boldsymbol{Y}$均为时序序列，通常称$\boldsymbol{X}$为观测序列，$\boldsymbol{Y}$为状态序列 --- 不可观测序列。其中观测序列/状态序列均可视为随机变量序列（注意，同一时刻的观测变量和状态变量并不是独立的），观测变量序列的取值为可观测值，状态变量序列的取值为状态值 --- 与马尔可夫模型中的状态含义一致。隐马尔科夫模型的假设：
\begin{itemize}
	\item 当前状态变量$\boldsymbol{Y}_t$的取值仅以来与前一个状态变量$\boldsymbol{Y}_{t-1}$的取值有关，连续多个状态变量的取值则构成隐马尔科夫链
	\item 任意时刻的观测变量$\boldsymbol{X}_t$取值仅依赖于该时刻的状态变量的取值$\boldsymbol{Y}_t$
\end{itemize}

一个隐马尔科夫模型可以表示为：$\lambda = (\boldsymbol{\pi}, \boldsymbol{A}, \boldsymbol{B})$，含义如下：
\begin{itemize}
	\item $\boldsymbol{\lambda}$：初始状态概率向量，即初始时刻的状态变量$\boldsymbol{Y}_0$取值为各个状态的概率
	\item $\boldsymbol{A}$：状态转移概率矩阵，即上一时刻的状态变量取某值时，当前时刻状态变量取值的概率分布：$p(\boldsymbol{Y}_t=s_j | \boldsymbol{Y}_{t-1}=s_i)$
	\item $\boldsymbol{B}$：发射概率矩阵，即当前状态变量取某值时观测变量取某观测值的概率分布：$p(\boldsymbol{X}_t=o_j | \boldsymbol{Y}_t=s_j)$
\end{itemize}

其实，可以看出马尔可夫模型是隐马尔科夫模型的一个特例 --- 状态变量序列与观测变量序列相同，状态值即为观测值，每个状态只对应一个观测值即本身。

隐马尔可夫模型的三个应用：
\begin{itemize}
	\item 样本生成问题：给定模型$\lambda = (\boldsymbol{\pi}, \boldsymbol{A}, \boldsymbol{B})$，生成满足模型约束的样本 --- 观测序列即对应的状态序列
	\item 模型的训练：给定训练数据 --- 观测序列即对应的状态序列，估计模型参数$\lambda = (\boldsymbol{\pi}, \boldsymbol{A}, \boldsymbol{B})$
	\item 序列预测：已知模型参数$\lambda = (\boldsymbol{\pi}, \boldsymbol{A}, \boldsymbol{B})$，给定观测序列，求状态序列
\end{itemize}

怎么解决上述的三个问题呢？

对于样本生成问题，其实很简单，指要逐步采样状态，得到一个状态序列，在根据每一步的状态取值采样得到观察值，就可以得到观测序列，样本生成就完成了。

对于模型的训练问题，需要估计的参数为：$(\boldsymbol{\pi}, \boldsymbol{A}, \boldsymbol{B})$，对于$\boldsymbol{\lambda}$则统计所有的状态序列中，计算以每个状态为开头的序列的频率即可，其他两个概率矩阵的训练类比即可。

对于序列预测问题可以通过维特比算法解决。给定一个观测序列，求解最有可能的状态序列。本质上这是一个搜索问题，搜索最有可能的状态序列，使观测序列的似然概率最大。简要地说一下。

维特比算法通过动态规划的方法解决这个问题。假设我们已经有最优的状态路劲，那么其中一条从起点开始的子路径也是最优的子路径。因此可以通过维护两个动态规划的矩阵来记录路径的选择和其概率。假设有两个矩阵$\boldsymbol{\sigma}, \boldsymbol{\psi}$。其中$\boldsymbol{\sigma}_{ti}$表示在时刻$t$时以$s_i$结尾的所有局部路径的最大概率，$\boldsymbol{\psi}_{ti}$表示在时刻$t$时末状态为$s_i$的前驱状态。

\paragraph{应用}使用隐马尔科夫模型可以解中文分词问题。给定训练数据 --- 每个样本为一个句子，对于每个样本，目标是给每个词分配一个标签（如{B, M, E, S}），然后可以根据序列对句子进行分词。为达成这个目的，需要训练隐马尔科夫模型，获得模型的各个参数，根据训练数据是否被标注可以使用不同的方法进行训练。获得训练数据后即可使用模型进行序列预测任务，再根据序列进行分词。


\colorbox{red}{注：本节内容主要参考何晗所著《自然语言处理入门》}。


\subsubsection{概率图模型}
将联合概率分布以图的形式来表示，可以分为有向概率图模型和无向概率图模型。
概率图模型中，将随机变量表示为结点，相关联的随机变量之间会有边连接。

有向图模型中按变量的因果关系进行连接，可以刻画变量之间的因果关系。有向图表示的联合概率可以根据图中的变量依赖关系分解成多个条件概率的积。（听着有点像拓扑图）

无向图模型没有体现出因果关系，强调的是变量之间的相互关系。无向图模型将联合概率分布分解为“无向图中-最大团上-随机变量的函数-的乘积”。

\subsubsection{命名实体识别}
NLP中的命名实体识别（Named Entity Recognition）指的是将识别出文本中描述实体的词汇，比如人名、地名、组织机构名、股票基金、医学术语等，更一般地讲，不同的领域会有不同的实体，用于描述该领域内的实体对象。NER的目的就是识别出文本中实体与其他文本的边界和实体的类别。主要以下几个难点：
\begin{itemize}
	\item 实体的数量是无穷的。不同的领域会有不同的实体，且实体并不是定量的，会不断的增加
	\item 实体的构词灵活。同一个实体可能会有多个名字，比如实体的简写；实体也可能是嵌套形成的
	\item 类别模糊。同一个实体在不同的语境下可能会有不同的类别
\end{itemize}

命名实体的识别，从某个角度来看可以视作一个序列标注问题。具体做法是将命名实体附着到{B, M, E, S}标签。其实，词性标注问题也可以以这样的方法来处理。更一般地看，词性标注与命名实体识别是同一个问题，它们都需要对文本进行分词，词性标注中的词性和NER中的实体类别是等价地。

\subsubsection{TF-IDF}
词频-倒排文档频次。通常用于衡量一个词语在文档中的重要程度。单单使用词频评价词语重要程度是不全面的，有些词可能会在文档中出现很多次，但是如果其在很多文档中都出现，却又体现不了其重要性。因此，需要一个对词频进行扩充，希望得到的重要的词应该是这样的：词频高，同时又不是出现在大部分文档中。
$$
TF-IDF(t, d) = \frac{TF(t, d)}{DF(t)} = TF(t, d) \cdot IDF(t)
$$
其中，$TF(t, d)$表示词$t$在文档$d$中出现的次数，$DF(t)$表示包含词$t$的文档数。实际中计算$TF-ID$时会加入一些平滑操作（如加一平滑、对IDF取对数）防止结果下溢等。 IDF表示inverse document-frequency，计算方法：
$$
IDF(t) = \log \frac{1 + n}{1 + DF(t)}
$$
\begin{center}
	或
\end{center}
$$
IDF(t) = \log \frac{n}{1 + DF(t)} + 1
$$
计算得到一个矩阵，每一行表示一个文档的tf-idf向量，每个元素表示对应的词（term）在该文档种的重要程度，一般还会进行$l2$正则化。

\subsubsection{词袋模型}
用于表示文档/句子（其实句子可以看作只有一个句子的文档）的一种方法。
词袋模型一般会先构建一个词表，每个文档经过分词后，可以使用不同的统计量来构建文档/句子的向量，词表之外的词不予考虑。可以选用的不同的统计量有：
\begin{itemize}
	\item 词频
	\item 布尔词频
	\item TF-IDF
	\item 词向量
\end{itemize}

\subsubsection{使用朴素贝叶斯分类器进行文本分类}
首先，给定类别数为$c$，每个文档的特征向量长度为$n$。

朴素贝叶斯分类器是一个生成式的分类器，其计算的是$p(Y | X)$，对于生成模型，重要的是要学习联合分布$p(X, Y)$，朴素贝叶斯通过训练数据学习该联合分布：$p(X, Y) = p(Y)p(X|Y)$。对于给定样本$X^i$，对其进行分类可形式化为：
$$
y = \mathop{argmax}_{c_k} p(Y=c_k | X=X^i) = \mathop{argmax}_{c_k} \frac{p(X=X^i | Y=c_k) p(Y=c_k)}{p(X=X^i)}
$$
朴素贝叶斯中假设样本的特征之间是相互独立的，即$p(X)=p(X_1=x_1, ..., X_n=x_n) = p(X_1=x_1) ... p(X_n=x_n)$。那么为了得到样本$X^i$的类别，只需要计算出$p(X=X^i | Y=c_k),  p(Y=c_k), p(X=X^i)$即可。而对于某一个样本的分类，$p(X^i)$对分类是没有帮助的，在$Y$取不同类别时是不会变化的，故可以不计算$p(X^i)$。那么剩下的需要计算的只有出$p(X=X^i | Y=c_k),  p(Y=c_k)$了。

$p(Y=c_k)$是很好计算的，只需要统计训练数据中类别为$c_k$的样本所占的比例即可。由于特征之间独立的假设，所以$p(X^i | Y=c_k) = \prod_{j=1}^{n} p(X_{j}^{i}=x_j | Y=c_k)$。这个概率也只需要从训练数据中统计即可，统计每个类别下，特征向量的第$j$维取$x_j$的比例即可。

关于样本的特征向量：通常可以采用词袋模型的方法来表示一个文本/句子。

\subsubsection{语法分析}
分析句子中的语法结构并将其表示为容易理解的结构（通常为树形结构）。
\paragraph{短语结构树}根据上下文无关文法将句子/短语分解为树状结构。短语结构语法（上下文无关文法）描述了如何自定向下地生成一个句子，同样句子/短语也可以通过短语结构语法进行分解。这其实和编译原理中的上下文无关文法是很类似的，也是通过很多产生式进行推导，那么是否可以使用编译原理中的词法分析来解决这个问题呢？

\paragraph{依存句法树}关注的是句子中词语之间的语法关联系，并将其约束为树形结构。在句子中，如果一个词语修饰另一个词语，则称修饰词为从属词，被修饰词为支配词，两者之间的语法关系称为依存关系，在可视化时，由支配词指向从属词。依存句法树描述了句子中的各个词之间的依赖关系，一般约定同一个词不能依存于多个词。

\textbf{复合性原理}：一个复杂表达式的意义是由其各组成部分的意义以及结合它们的规则决定的。通过将句子分解为短语、分解短语为单词，下游应用会得到更多更深层次的结构化信息。
