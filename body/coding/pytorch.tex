\subsection{pytorch tensor.view}
\href{https://pytorch.org/docs/stable/tensor_view.html}{view()}相当于数据库中的view --- 对数据进行查看, 是查看数据的一种方式. 使用view()不会产生数据的复制, 与原tensor共享同一块内存, 修改view会使原tensor发生变化. 

\subsection{nn.Dropout}\label{nn_dropout}
\mintinline{python}{torch.nn.Dropout(p=0.5, inplace=False)}. \mintinline{python}{nn.Dropout}实现了Dropout, 可以作为神经网络中的层来使用. 对于输入\mintinline{python}{nn.Dropout}的数据, 会以参数为$p$的伯努利分布对每个channel的数据进行采样然后置零（每个channel中的每个元素有$p$的概率置零）. \textbf{注意: }在训练时, \mintinline{python}{nn.Dropout}的输出会乘以$\frac{1}{1 - p}$进行缩放, 不训练时, 则等于恒等映射. 不管是否有元素被置零, 都会乘以$\frac{1}{1 - p}$, $p$越大, 放大的倍数越大. 

\subsection{torch.nn.CrossEntropyLoss}
torch中的交叉熵损失, 声明为: 
\begin{minted}[autogobble, breaklines]{python}
	class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction='mean', label_smoothing=0.0)
\end{minted}

该类实现了交叉熵损失函数的计算, 有以下注意点: 
\begin{myitemize}
	\item 调用时, 第一个参数是预测值, 第二个参数是真实值；
	\item 预测值是\textbf{未归一化的类别分数}；
\end{myitemize}
