\subsection{基于人口统计学的推荐}
特征相似的人喜欢的东西应该也类似. 根据\textbf{用户画像}（以用户的基本信息作为相似度计算的基础）, 找出与目标用户相似的用户, 将相似用户喜欢的物品推荐给目标用户. 这种方法需要构建用户画像. 
\paragraph{优点}
\begin{itemize}
	\item 不涉及当前用户的历史喜好, 所以解决了“\textbf{用户冷启动}”问题
	\item 不依赖于物品本身的数据, 故而无论这个物品是“书籍”、“音乐”还是“短视频”都可以使用, 即它是领域独立的
\end{itemize}

\paragraph{缺点}
\begin{itemize}
	\item 用户画像所需要的有些数据难以获取
	\item 以人口统计信息计算相似用户不可靠. 特别是“书籍”“音乐”这种涉及到个人喜好的商品, 单用这种方法更是难以达到很好的效果
\end{itemize}

\subsection{基于内容的推荐}
把用户可能喜欢的物品类型进行推荐, 即要找到相似的物品. 需要构建物品的特征, 如对物品进行标签化. 
\paragraph{优点}
\begin{itemize}
	\item 不存在稀疏性和“\textbf{项目冷启动}”问题
	\item 简单有效, 推荐结果具有可解释性, 不需要领域知识
	\item 基于物品本身特征推荐, 不存在过度推荐热门的问题
	\item \textbf{解决了基于人口统计学对个人兴趣建模的缺失}, 能够很好的建模用户的喜好, 实现更精确的推荐
\end{itemize}

\paragraph{缺点}
\begin{itemize}
	\item 推荐的结果\textbf{没有新颖性}
	\item 由于需要基于用户的兴趣偏好进行推荐, 故而存在“\textbf{用户冷启动}”问题
	\item 该方法受推荐对象特征提取能力的限制. 由于是根据物品相似度进行推荐, 故而, 物品特征构建模型的完善和全面决定了最后推荐的质量, 然而像图像、音频等这种类型的特征难以提取
\end{itemize}

\subsection{基于协同过滤的推荐}
Collaborative Filtering, 通过群体的行为来寻找相似性（用户或物品的相似性）, 通过该相似性来做推荐, 现在通常用于做召回. 协同过滤算法可以分为以下几类: 
\paragraph{基于用户的协同过滤}
User-based CF（UserCF）, 根据用户对物品的偏好, 发现与当前用户口味和偏好相似的“邻居”用户群, 并推荐近邻所偏好的物品, 重点在于得到\textbf{用户之间的相似性}. 与基于人口统计学的推荐的联系与区别: 
\begin{itemize}
	\item 都是基于相似用户来推荐
	\item 不同之处在于如何计算用户相似度: 基于人口统计学只考虑用户本身的特征, 而UserCF是在用户的历史偏好的数据上计算用户的相似度, 它的基本假设是, 喜欢类似物品的用户可能有相同或者相似的偏好
\end{itemize}
特点: 
\begin{itemize}
	\item 适用于用户数较小的场景
	\item 适用于时效性较强（即物品变化频繁）, 用户个性化兴趣不太明显的领域, 如新闻推荐
	\item 在新用户对很少的物品产生行为后, 不能立即对它进行个性化推荐, 因为用户相似度表示每隔一段时间离线计算的, 故而存在“用户冷启动”问题	
	\item 新物品上线后一段时间, 一旦有用户对物品产生行为, 就可以将新物品推荐给和对它产生行为的用户兴趣相似的其他用户, 故而解决了“项目冷启动”问题
	\item 解释性较差, 因为计算得到的相似用户可能并不是真的相似, 并不能真的反映用户的兴趣. 例如, 用户都买了卫生纸、水杯等日常用品并不能表示用户之间相似, 但如果都买了键盘、屏幕等, 则能较可靠的推断用户是相似的, 因此在计算用户相似度时要注意这种大家都会关注的“热门物品”, 大家都选择热门物品并不能体现用户的真实兴趣（可以依据社交网络来防止相似用户并不相似）
\end{itemize}

\paragraph{基于物品的协同过滤}
Item-based CF（ItemCF）, 基于用户对物品的偏好, 发现物品和物品之间的相似度\textbf{物品之间的相似度}, 然后根据用户的历史偏好信息, 将类似的物品推荐给用户. 与基于内容的推荐的联系与区别: 
\begin{itemize}
	\item 都是基于相似物品进行推荐
	\item 不同之处在于如何计算物品相似度: ItemCF是根据用户历史的偏好（如共现矩阵）推断, 而基于内容的推荐是基于物品本身的属性特征
\end{itemize}
特点: 
\begin{itemize}
	\item 适用于物品数明显小于用户数的场合, 如果物品很多, 计算物品的相似度矩阵的代价就会很大
	\item 适合于长尾物品丰富, 用户个性化需求强烈的领域, 如电商网站
	\item 用户有新行为, 一定会导致推荐结果的实时变化
	\item 新用户只要对一个物品产生行为, 就可以给它推荐和该物品相关的其它物品, 故而解决了“用户冷启动”问题
	\item 不能在不离线更新物品相似度的情况下将新的物品推荐给用户, 故而存在“项目冷启动”问题
	\item 有较强的解释性, 因为是依据用户历史偏好的物品来推荐
\end{itemize}


\paragraph{基于模型的协同过滤}
Model-based CF（ModelCF）. 基本思想: 用户具有一定的特征, 决定着他的偏好选择；物品具有一定的特征, 影响着用户需是否选择它；用户之所以选择某一个商品, 是因为用户特征与物品特征相互匹配. ModelCF基于样本的用户偏好信息, 训练一个推荐模型, 然后根据实时的用户喜好的信息进行预测, 计算推荐. 

参考: \href{https://www.cnblogs.com/shengyang17/p/11516532.html}{基于协同过滤的推荐算法}、\href{https://zhuanlan.zhihu.com/p/108759393}{推荐系统——经典算法（基于内容、协同过滤、混合等）}. 




\subsection{FM}
Factorization Machine\cite{rendle_fm_2010_icdm}, 因子分解机. FM 在线性模型的基础上增加了二阶的特征交叉. 且并不是直接去学习每一个交叉特征的权重, 而是通过将二阶特征的权重矩阵进行分解, 为每个特征学习一个隐向量, 用隐向量的内积作为交叉特征的权重. 先上公式: 
$$
\hat{y}(\boldsymbol{x}) = w_0 + \sum_{i=1}^n w_i \cdot x_i + \sum_{i=1}^n \sum_{j = i+1}^n <\boldsymbol{v}_i, \boldsymbol{v}_j>x_i \cdot x_j 
$$

其中: $\boldsymbol{v}_i \in \mathbb{R}^k, \boldsymbol{V} \in \mathbb{R}^{n \times k}$, $\boldsymbol{v}_i$ 是 $\boldsymbol{V}$ 的第 $i$ 行. 数据集 $D = \{ (\boldsymbol{x}^{(1)}, y^{(1)}), ... \}, \boldsymbol{x}^{(i)} \in \mathbb{R}^n$. 

如果直接按照上式的双重循环进行计算的话, 那么时间复杂度将是 $O(kn^2)$. 对 FM 的公式进行化简, 主要针对 $\sum_{i=1}^n \sum_{j = i+1}^n <\boldsymbol{v}_i, \boldsymbol{v}_j>x_i \cdot x_j$: 
$$
\begin{aligned}
	\sum_{i=1}^n \sum_{j = i+1}^n <\boldsymbol{v}_i, \boldsymbol{v}_j>x_i \cdot x_j &= \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n <\boldsymbol{v}_i, \boldsymbol{v}_j>x_i \cdot x_j - \frac{1}{2} \sum_{i=1}^n <\boldsymbol{v}_i, \boldsymbol{v}_i>x_i^2 \\
	&= \frac{1}{2} ( \sum_{i=1}^n \sum_{j=1}^n \sum_{f=1}^k v_{if}\cdot v_{jf} \cdot x_i \cdot x_j - \sum_{i=1}^n \sum_{f=1}^k v_{if}^2 \cdot x_i^2) \\ 
	&= \frac{1}{2} \sum_{f=1}^k ( \sum_{i=1}^n \sum_{j=1}^n v_{if}\cdot v_{jf} \cdot x_i \cdot x_j - \sum_{i=1}^n v_{if}^2 \cdot x_i^2)  \\
	&= \frac{1}{2} \sum_{f=1}^k ( (\sum_{i=1}^n v_{if} \cdot x_i)  (\sum_{j=1}^n v_{jf} \cdot x_j) - \sum_{i=1}^n v_{if}^2 \cdot x_i^2)  \\
	&= \frac{1}{2} \sum_{f=1}^k ( (\sum_{i=1}^n v_{if} \cdot x_i)^2 - \sum_{i=1}^n v_{if}^2 \cdot x_i^2)
\end{aligned}
$$

化简后, 时间复杂度直降为 $O(kn)$.

用 $l(y, \hat{y})$ 表示损失函数, 则对样本 $(\boldsymbol{x}, y)$ 的损失进行求导: 
$$
\frac{\partial l}{\partial \theta} = \frac{\partial l}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial \theta}
$$
其中 $\theta$ 为参数, 对不同的参数有: 
$$
\frac{\partial \hat{y}}{\partial \theta} = 
\begin{cases}
	1 & \theta = w_0 \\
	x_i & \theta = w_i, 1 \leq i \leq n \\
	x_i \sum_{j=1}^n v_{jf}\cdot x_j - v_{if} x_i^2 & \theta = v_{if}
\end{cases}
$$

FM 的矩阵形式, 令 $\boldsymbol{X} \in \mathbb{R}^{m \times n}$ 表示 $m$ 个样本, 为一批, 如何用 FM 对一批数据 $\boldsymbol{X}$ 进行处理呢？
$$
\hat{\boldsymbol{Y}} = w_0 + \boldsymbol{X} * \boldsymbol{w} + \frac{1}{2} \{ [(\boldsymbol{X} * \boldsymbol{V})^2 - (\boldsymbol{X} \cdot \boldsymbol{X}) * (\boldsymbol{V} \cdot \boldsymbol{V})].sum(axis=1) \}
$$
上式中, $*$ 为矩阵乘法, $\cdot$ 为内积或标量乘法. 

关于求导的矩阵推导: \href{https://blog.csdn.net/csuyhb/article/details/100575149}{FM模型理论之---矩阵形式解读}. 


\paragraph{为什么说 FM 能够处理稀疏的数据呢?}
其实这个问题也是在问为什么不直接去计算交叉特征的权重. 如果采取直接计算的方式, 那么对于权重 $w_{ij}$, 只有在 $x_i x_j$ \textcolor{red}{\textbf{都不等于 0}} 时才能更新 $w_{ij}$, 这样的话参数的计算就会受到 $x_i, x_j$ 能否同时取非零值的影响 --- 数据稀疏的影响. 对权重矩阵分解后, 只要 $x_i$ 或 $x_j$ 不为 0, 则对应特征的隐向量都会更新. 虽然还是会受到单个特征的稀疏性的影响, 但是相较于交叉特征的稀疏性还是要好不少的.

\paragraph{为什么要对权重矩阵进行分解呢?}
很显然, 能够缓解数据稀疏的问题. 其次, 在分解前, $x_i$ 与其他特征的交叉特征的权重学习过程是独立的. 分解之后则不然, 交叉特征的权重与 $\boldsymbol{v}_i$ 都是有关系的, 能够提高模型的泛化能力. 再者, 可以优化计算过程, 即计算的时间复杂度降为 $O(kn)$.

\subsubsection{FM 做排序}

这个做法比较简单, 将样本的特
征输入计算得到一个分数 (例如点击率), 作为排序的依据.


\subsubsection{FM 做召回}

这个是我比较疑惑的地方. 以推荐的场景为例, 可以将输入的特征分成三类: 用户侧特征, 物料侧特征, 上下文特征. 在做召回时, 可以把上下文特征作为用户侧的特征一起考虑, 因此可以只看做两类: 用户侧和物料侧特征. 假设用户侧特征共有 n 个, 为 $u_1, ..., u_n$, 物料测共有 m 个特征, 为 $v_1, ..., v_m$. 那么在为一个用户召回时, 其实也是看物料与用户的匹配的程度, 即 FM 计算得到的分数:
$$
\begin{aligned}
	y = &w_0 + \sum_{i=1}^n w^u_i u_i + \sum_{j=1}^m w^v_j v_j \\
	 	&+ \sum_{i=1}^n \sum_{j=i+1}^n <\boldsymbol{u_i}, \boldsymbol{u_j}> u_i u_j + \sum_{i=1}^m \sum_{j=i+1}^m <\boldsymbol{v_i}, \boldsymbol{v_j}> v_i v_j \\
	 	&+ \sum_{i=1}^n \sum_{j=1}^m <\boldsymbol{u_i}, \boldsymbol{v_j}> u_i v_j
\end{aligned}


$$ 

上式所表达的含义即: 分数 = 用户侧的一阶 + 物料测的一阶 + 用户侧的二阶 + 物料测的二阶 + 用户与物料的交互. 在训练阶段, 可以通过点击率等任务来训练参数以得到模型的参数: 权重以及特征的隐向量. 得到上述参数后, 为具体的一个用户做召回时, 其实就是计算候选物料池中的物料与用户的匹配分数. 那还需要代入上面的公式完整地计算一遍吗? 并不需要. 为同一个用户召回时, 用户侧的一阶以及用户侧的二阶是没必要计算的, 这对所有物料而言都是一样的. 因此, 分数 = 物料侧的一阶 + 物料侧的二阶 + 用户与物料的交互. 即:

$$
\hat{y} = \sum_{j=1}^m w^v_j v_j + \sum_{i=1}^m \sum_{j=i+1}^m <\boldsymbol{v_i}, \boldsymbol{v_j}> v_i v_j + \sum_{i=1}^n \sum_{j=1}^m <\boldsymbol{u_i}, \boldsymbol{v_j}> u_i v_j
$$

我们来对最后一项 $\sum_{i=1}^n \sum_{j=1}^m <\boldsymbol{u_i}, \boldsymbol{v_j}> u_i v_j$ 进行化简:

$$
\begin{aligned}
	\sum_{i=1}^n \sum_{j=1}^m <\boldsymbol{u_i}, \boldsymbol{v_j}> u_i v_j &= \sum_{i=1}^n \sum_{j=1}^m \sum_{f=1}^k  u_{i,f} v_{j,f} u_i v_j \\
		&= \sum_{f=1}^k \sum_{i=1}^n \sum_{j=1}^m  u_{i,f} v_{j,f} u_i v_j \\ 
		&= \sum_{f=1}^k ( u_{1,f} v_{1,f} u_1 v_1 + u_{1,f} v{2, f} u_1 v_{2} + ... + u_{1,f} v_{m,f} u_1 v_m \\
		&\quad\quad\quad + u_{2, f} v_{1f} u_{2} v_1 + ... + u_{2, f} v_{m, f} u_{2} v_m \\
		&\quad\quad\quad + ... + u_{n, f} v_{1, f} u_n v_1 + ... + u_{n, f} v_{m, f} u_n v_m) \\
		&= \sum_{f=1}^k ( u_{1,f}u_1 \sum_{j=1}^m v_{j,f} v_j + u_{2, f} u_2 \sum_{j=1}^m v_{j, f} v_j + ... + u_{n, f} u_n \sum_{j=1}^m v_{j, f} v_j ) \\
		&= \sum_{f=1}^k (\sum_{i=1}^n u_{i,f} u_i \sum_{j=1}^m v_{j, f} v_j) \\
		&= <\sum_{i=1}^n \boldsymbol{u}_i u_i, \sum_{j=1}^m \boldsymbol{v}_j v_j>
\end{aligned}
$$

其中双下标或者黑体的表示隐向量, 单下标的表示特征取值. 通过上式的化简, 可知用户和物料侧特征的交叉等价于: 聚合用户侧的特征隐向量后作为用户向量, 聚合物料侧的特征隐向量后作为物料向量, 再做内积. 我们把 $\sum_{j=1}^m w^v_j v_j + \sum_{i=1}^m \sum_{j=i+1}^m <\boldsymbol{v_i}, \boldsymbol{v_j}> v_i v_j$ 定义为物料侧得分, 另 $\boldsymbol{E}_{user} = [1,\ \sum_{i=1}^n \boldsymbol{u}_i u_i]$, $\boldsymbol{E}_{item} = [\text{物料侧得分},\ \sum_{j=1}^m \boldsymbol{v}_j v_j]$. 则 $\hat{y}$ 的求法:

$$
\hat{y} = <\boldsymbol{E}_{user},\ \boldsymbol{E}_{item}>
$$

或许还可以这样做. 在特征隐向量的基础上生成用户和物料的向量表征, 再进行向量召回. 

参考资料: \href{https://zhuanlan.zhihu.com/p/343174108}{FM：推荐算法中的瑞士军刀}.

\subsection{FFM}
Field-aware Factorization Machine\cite{juan_ffm_2016_recsys}, 域敏感的因子分解机. 

参考资料: \href{https://tech.meituan.com/2016/03/03/deep-understanding-of-ffm-principles-and-practices.html}{深入FFM原理与实践}.

\subsection{EGES}
来自淘宝 18 年在 KDD 上发表的论文 \href{https://arxiv.org/pdf/1803.02349.pdf}{Billion-scale Commodity Embedding for E-commerce	Recommendation in Alibaba}. 主要是为了解决淘宝的推荐中的三个问题: 
\begin{itemize}
	\item Scalability. 现有的推荐方法在百万级别的数据集上工作良好, 但在十亿级别的数据集上却效果不佳. 淘宝中的用户和物品都在十亿级别, 如何在如此大规模的数据上做推荐是一个很有挑战的问题;
	
	\item Sparsity. 由于用户倾向于与一小部分物品产生交互 --- 数据的稀疏性, 如何为这种交互行为少的用户或长尾物品做推荐?
	

	\item Cold Start. 在淘宝中, 每小时都有上百万的新增物品, 新增物品是没有交互记录的, 很难预测用户是否对这些新物品感兴趣.
\end{itemize}

为了解决以上问题, 淘宝设计了一个两阶段的推荐框架: 1) 匹配阶段. 根据用户的行为, 生成一个与历史交互相似的物品集合; 2) 排序. 根据用户的偏好, 训练一个深度模型对候选集的物品进行排序. 改论文研究的是匹配阶段的问题, 该阶段的核心问题就是\textbf{如何基于用户行为计算物品之间的相似性}.

\subsection{Swing}
顾名思义, swing 表示秋千. Swing 是一种基于用户和物品构成的二部图的推荐算法, 在这样的二部图中存在着类似 swing 的结构. Swing 通常可以用来计算物品之间的相似性, 其出发点是: 若用户 $u, v$ 都与物品 $i$ 有过交互, 且都与 $j$ 有过交互, 那么 $i, j$ 之间具有较强的关系, 即物品之间的相似性通过用户来传递. Swing 计算方式:

$$
sim(i, j) = \sum_{u \in U_i \cap U_j} \sum_{v \in U_i \cap U_j} \frac{1}{\alpha + | I_u \cap I_v |}
$$

其中 $U_i$ 表示与 $i$ 交互过的用户, $I_u$ 表示 $u$ 交互过的物品. 该公式的含义: 对于购买了 $i, j$ 的两个用户来说, ta 们共同购买的物品越少, 则说明 $i, j$ 越相似. 共同购买的越少说明 $u, v$ 的差异大, 但是却都买了 $i, j$, 说明 $i, j$ 有较高的相似性.  

实际计算式可以这样:
\begin{enumerate}
	\item 统计与 $i, j$ 都交互过的用户集, 并计算 $N = C_m^2$, $m = |U_i \cap U_j|$;
	
	\item 计算每个用户 pair 的值, 即 $w_i = \frac{1}{\alpha + |I_u \cap I_v|}$
	
	\item 计算相似性, $sim = \sum_N w_i$;
\end{enumerate}

简言之, 就是以用户行为为基础构建 Item graph, 然后使用 Skip-gram 学习物品的表征. 为了融入 \textit{side information} (通常是稀疏特征), 在训练的过程中同时学习 \textit{side information} 的表征, 并以加权的方法得到物品最终的表征. 